# Abstract

We present an information-theoretic clustering approach that decomposes a hierarchical tree into stable partitions through sequential hypothesis testing. The method employs local, data-driven stopping criteria to adapt to the specific structure of each branch, verifying child-parent divergence and sibling divergence to identify statistically distinct clusters.

## Introduction

The analysis begins with a hierarchical tree structure derived from the input dataset. While standard hierarchical clustering methods often rely on arbitrary global thresholds (e.g., a fixed cut height) to partition such trees, this approach **dynamically identifies stable partitions using local statistical tests**. It adapts to the specific structure of each branch and uses projected Wald chi-square tests for both child-parent and sibling comparisons, rather than relying on user-defined global cut parameters. The top-down decision process sequentially evaluates candidate splits—potential subtrees that represent distinct clusters—by verifying two properties: **Information Gain** (child-parent divergence) and **Sibling Divergence** (whether the two children are statistically separable).

## Method

The method models the input data ($N$ samples, $D$ **features**) as a probabilistic hierarchy. It begins with a candidate structure—formally a **rooted, binary, leaf-labeled tree**—where leaves correspond bijectively to input samples and internal nodes represent nested candidate groups. This skeleton, typically generated by standard hierarchical clustering (e.g., Agglomerative Linkage), forms the basis for the analysis. In this representation, every node $u$ is associated with a probability distribution vector $\theta_u$ (e.g., Bernoulli, Categorical, or Poisson), calculated by averaging the feature vectors of all leaf descendants to provide a probabilistic summary of the subtree.

## 2. The Core Algorithm: Tree Decomposition

The algorithm proceeds recursively from the root to the leaves. If a node passes all statistical gates, it is considered a **heterogeneous mixture**, and the splitting process continues to its children to find finer-grained substructures. However, if a node fails any gate, the split is deemed **not significant**, indicating the node represents a single homogeneous group. The splitting process stops at that branch, and the current node is identified as a final **Cluster**. All samples (leaves) within this node are assigned to this cluster, effectively grouping them together. This process naturally prunes the tree, resulting in a final partition of the dataset into statistically stable, homogeneous groups.

A node is split only if it passes two complementary gates. The first gate is a **Local Child-Parent Divergence Test** that checks whether a child node is statistically distinguishable from its parent. We first compute probabilistic summaries for each node. Specifically, for a node $u$ containing a set of descendant samples $S_u$, we calculate the mean feature vector to estimate distribution parameters $\theta_u$:
$$ \theta_u = \frac{1}{|S_u|} \sum_{i \in S_u} x_i $$
where $x_i$ represents the feature vector of sample $i$. This defines the parent ($\theta_{parent}$) and child ($\theta_{child}$) distributions.

For each edge $e=(p \rightarrow c)$, the implementation uses a nested-sample Wald standardization:
$$
z_j = \frac{\theta_{c,j} - \theta_{p,j}}{\sqrt{\theta_{p,j}(1-\theta_{p,j})\left(\frac{1}{n_c}-\frac{1}{n_p}\right)}}
$$
where $n_c < n_p$ is required by the tree nesting structure.

When branch lengths are available, the edge variance is scaled by a Felsenstein-style normalized factor:
$$
\mathrm{Var}_{adj} = \mathrm{Var}\cdot\left(1+\frac{b_e}{\bar b}\right),
$$
where $b_e$ is the edge branch length and $\bar b$ is the mean valid branch length in the tree.

For categorical features, one category per feature is removed before testing to respect simplex constraints (effective $K-1$ dimensions per feature). The standardized vector is then projected to dimension $k$ (chosen by a Johnson-Lindenstrauss bound), and the edge statistic is:
$$
T_{edge} = \|Rz\|_2^2,\qquad T_{edge}\sim\chi^2_k \text{ under } H_0.
$$
The p-value is computed as:
$$
p_e = \Pr(\chi^2_k \ge T_{edge}).
$$

Since the algorithm performs many edge tests recursively, false discoveries can accumulate. To control this, an FDR correction is applied to edge-level p-values (TreeBH by default in the current implementation, with flat BH and level-wise variants also available). For flat BH, we sort the $m$ p-values in ascending order
$$ p_{(1)} \le p_{(2)} \le \dots \le p_{(m)} $$
and identify the largest rank $k$ satisfying the condition below. This rank $k$ corresponds to the index of the largest p-value that is considered significant:

$$ p_{(k)} \le \frac{k}{m} \alpha $$

This threshold $\frac{k}{m} \alpha$ acts as the adjusted critical value. If an edge p-value is less than or equal to this threshold, the child-parent divergence is marked **significant**.

Let $\mathcal{E}$ be the set of all edges in the tree. We identify the subset of significant edges $\mathcal{E}_{sig} \subseteq \mathcal{E}$ by evaluating the local divergence hypothesis:
$$ \mathcal{E}_{sig} = \{ e \in \mathcal{E} \mid \text{Reject } H_{0,\text{edge}}(e) \} $$
We then proceed to the **Sibling Divergence Test**, which is evaluated conditionally on the structure retained in $\mathcal{E}_{sig}$.

This test evaluates whether the two children ($C_1$ and $C_2$) of the current node $P$ are statistically separable. In the implementation, sibling testing is a two-sample projected Wald test: compute standardized coordinate-wise differences, optionally apply branch-length variance scaling, project to dimension $k$ (chosen via the Johnson-Lindenstrauss bound), and evaluate
$$ T_{sib} = \|R z\|_2^2 \sim \chi^2_k $$
under the null of equal sibling distributions. For categorical features, one category per feature is dropped before projection to respect simplex constraints. Sibling p-values across eligible parents are then corrected with BH/FDR at level $\alpha$, and a split is retained only when siblings are significantly different after correction.

##### Conclusion

This approach constitutes a statistically grounded pruning of a hierarchical tree. By combining information-theoretic node summaries with projected Wald divergence tests and FDR control, it ensures that the resulting clusters are both distinct from their neighbors and internally homogeneous, without relying on arbitrary global thresholds.
