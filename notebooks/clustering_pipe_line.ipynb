{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6abc0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "spec not found for the module 'kl_clustering_analysis.plot.validation_visualizations'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module_name \u001b[38;5;129;01min\u001b[39;00m modules_to_reload:\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module_name \u001b[38;5;129;01min\u001b[39;00m sys.modules:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m         \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     24\u001b[39m         \u001b[38;5;28m__import__\u001b[39m(module_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/importlib/__init__.py:128\u001b[39m, in \u001b[36mreload\u001b[39m\u001b[34m(module)\u001b[39m\n\u001b[32m    126\u001b[39m spec = module.__spec__ = _bootstrap._find_spec(name, pkgpath, target)\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mspec not found for the module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m, name=name)\n\u001b[32m    129\u001b[39m _bootstrap._exec(spec, module)\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# The module may have replaced itself in sys.modules!\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: spec not found for the module 'kl_clustering_analysis.plot.validation_visualizations'"
     ]
    }
   ],
   "source": [
    "# Setup Python path to import from project root\n",
    "import sys, os, glob, importlib\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from kl_clustering_analysis.benchmarking import benchmark_cluster_algorithm\n",
    "from tests.test_cases_config import SMALL_TEST_CASES\n",
    "from datetime import datetime\n",
    "\n",
    "modules_to_reload = [\n",
    "    'kl_clustering_analysis.hierarchy_analysis.statistics.kl_tests.edge_significance',\n",
    "    'kl_clustering_analysis.hierarchy_analysis.statistics.cmi_tests.conditional_sibling_independence',\n",
    "    'kl_clustering_analysis.hierarchy_analysis.statistics',\n",
    "    'kl_clustering_analysis.hierarchy_analysis.tree_decomposer',\n",
    "    'kl_clustering_analysis.tree.poset_tree',\n",
    "    'kl_clustering_analysis.plot.cluster_tree_visualization',\n",
    "    'kl_clustering_analysis.benchmarking.pipeline',\n",
    "]\n",
    "\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        importlib.reload(sys.modules[module_name])\n",
    "    else:\n",
    "        __import__(module_name)\n",
    "\n",
    "print(\"All modules reloaded successfully\")\n",
    "\n",
    "\n",
    "# Clean up previous results in the results folder\n",
    "results_folder = \"../cluster_tree_plots\"\n",
    "if os.path.exists(results_folder):\n",
    "    files = glob.glob(os.path.join(results_folder, \"*.png\"))\n",
    "    for f in files:\n",
    "        try:\n",
    "            os.remove(f)\n",
    "            print(f\"Deleted previous result: {f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting {f}: {e}\")\n",
    "else:\n",
    "    os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "# Run validation with SMALL test cases and UMAP plotting enabled\n",
    "df_results, fig = benchmark_cluster_algorithm(\n",
    "    significance_level=0.05,\n",
    "    verbose=True,\n",
    "    plot_umap=True,\n",
    ")\n",
    "\n",
    "# Save validation results to results folder\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "results_file = f\"../results/validation_results_{current_date}.csv\"\n",
    "df_results.to_csv(results_file, index=False)\n",
    "print(f\"Validation results saved to {results_file}\")\n",
    "print(f\"Results summary:\\n{df_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad08532",
   "metadata": {},
   "source": [
    "# Research: Better Permutation Test Sample Size Approximation\n",
    "\n",
    "## Current Implementation Issues\n",
    "\n",
    "The current code uses a simple heuristic:\n",
    "```python\n",
    "min_resolution = max(significance_level_alpha / 10.0, 1e-4)\n",
    "min_required_perms = int(np.ceil((1.0 / min_resolution) - 1))\n",
    "effective_permutations = max(int(permutations), max(200, min_required_perms))\n",
    "```\n",
    "\n",
    "This approach has limitations:\n",
    "1. **Oversimplification**: Uses `1/resolution - 1` which doesn't account for statistical variability\n",
    "2. **Conservative but inefficient**: May require excessive permutations for accurate p-value estimation\n",
    "3. **No adaptive strategy**: Fixed number regardless of observed effect size\n",
    "\n",
    "## Modern Approaches from Literature\n",
    "\n",
    "### 1. **Generalized Pareto Distribution (GPD) Tail Approximation**\n",
    "**Reference**: Knijnenburg et al. (2009), \"Fewer permutations, more accurate P-values\", *BMC Bioinformatics*\n",
    "\n",
    "**Key Finding**: Generate permutations until M (number of permutation values exceeding test statistic) > 25, then fit GPD to the tail.\n",
    "\n",
    "**Advantages**:\n",
    "- Accurate p-value estimation with far fewer permutations (often 50-200 vs 10,000+)\n",
    "- Adapts to observed data\n",
    "- Works well for small p-values\n",
    "\n",
    "**Implementation Strategy**:\n",
    "```python\n",
    "# Stop when at least 25-30 extreme values observed\n",
    "# Fit GPD to tail of permutation distribution\n",
    "# Extrapolate p-value from fitted distribution\n",
    "```\n",
    "\n",
    "### 2. **Sequential/Adaptive Permutation Testing**\n",
    "**Reference**: Fay & Follmann (2002), \"Designing Monte Carlo implementations of permutation tests\"\n",
    "\n",
    "**Approach**:\n",
    "- Start with small number of permutations (e.g., 100-200)\n",
    "- If p-value is clearly significant or non-significant, stop early\n",
    "- Otherwise, continue with more permutations\n",
    "- Use confidence intervals on p-value estimate to guide stopping\n",
    "\n",
    "**Formula for CI on p-value**:\n",
    "For B permutations with X extreme values:\n",
    "$$\\hat{p} = \\frac{X + 1}{B + 1}$$\n",
    "$$SE(\\hat{p}) \\approx \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{B}}$$\n",
    "\n",
    "### 3. **Phipson & Smyth (2010) Correction**\n",
    "**Reference**: \"Permutation P-values should never be zero\", *Statistical Applications in Genetics*\n",
    "\n",
    "**Key Formula**:\n",
    "$$p = \\frac{b + 1}{n + 1}$$\n",
    "where b = number of permutation statistics ≥ observed statistic, n = total permutations\n",
    "\n",
    "**Advantage**: Provides unbiased p-value estimates even with few permutations\n",
    "\n",
    "### 4. **North et al. (2002) - Minimum Sample Size**\n",
    "**Recommendation**: For α = 0.05, minimum 1000 permutations; for α = 0.01, minimum 5000 permutations\n",
    "\n",
    "**Rule of thumb**: \n",
    "$$n_{min} = \\frac{20}{\\alpha}$$\n",
    "\n",
    "This ensures p-value resolution is at least 20× finer than the significance threshold.\n",
    "\n",
    "## Recommended Improved Implementation\n",
    "\n",
    "### Option A: Simple Improvement (North et al.)\n",
    "```python\n",
    "# More principled minimum based on statistical resolution\n",
    "min_perms_for_alpha = int(np.ceil(20 / significance_level_alpha))\n",
    "# Add buffer for BH correction (multiple testing)\n",
    "min_perms_with_buffer = int(min_perms_for_alpha * 1.5)\n",
    "effective_permutations = max(int(permutations), min_perms_with_buffer, 200)\n",
    "```\n",
    "\n",
    "### Option B: Advanced - GPD Tail Approximation\n",
    "```python\n",
    "# Implement adaptive permutation with GPD tail fitting\n",
    "# 1. Start with min_perms = max(200, 20/alpha)\n",
    "# 2. Continue until ≥25 extreme values observed OR reached max_perms\n",
    "# 3. Fit GPD to tail and extrapolate p-value\n",
    "# 4. Provides accurate p-values with fewer permutations\n",
    "```\n",
    "\n",
    "### Option C: Hybrid Approach (Recommended)\n",
    "```python\n",
    "# Use North et al. baseline with GPD option for expensive tests\n",
    "baseline_perms = max(200, int(np.ceil(20 / significance_level_alpha)))\n",
    "# For multiple testing, account for expected number of tests\n",
    "if use_benjamini_hochberg:\n",
    "    baseline_perms = int(baseline_perms * 1.5)  # 50% buffer for BH\n",
    "effective_permutations = max(int(permutations), baseline_perms)\n",
    "```\n",
    "\n",
    "## Implementation Notes\n",
    "\n",
    "1. **For your code** (sibling independence with Benjamini-Hochberg):\n",
    "   - With α = 0.05: `min_perms = 20/0.05 * 1.5 = 600` permutations\n",
    "   - With α = 0.01: `min_perms = 20/0.01 * 1.5 = 3000` permutations\n",
    "\n",
    "2. **Current calculation for α=0.05**:\n",
    "   - `min_resolution = max(0.05/10, 1e-4) = 0.005`\n",
    "   - `min_required_perms = ceil(1/0.005 - 1) = 199`\n",
    "   - This is close to North et al.'s 400, but lacks statistical justification\n",
    "\n",
    "3. **Improvement**: Replace with empirically validated formula from North et al.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Knijnenburg et al. (2009). \"Fewer permutations, more accurate P-values\". *BMC Bioinformatics*, 10:179\n",
    "2. North et al. (2002). \"A note on the calculation of empirical P values from Monte Carlo procedures\". *Am J Hum Genet*, 71(2):439-41\n",
    "3. Phipson & Smyth (2010). \"Permutation P-values should never be zero\". *Stat Appl Genet Mol Biol*, 9:Article 39\n",
    "4. Fay & Follmann (2002). \"Designing Monte Carlo implementations of permutation or bootstrap hypothesis tests\". *Am Stat*, 56(1):63-70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4763361f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARISON: Permutation Sample Size Requirements\n",
      "================================================================================\n",
      "Alpha      Current      North (base)    North (BH)      Ratio (BH/Current)\n",
      "--------------------------------------------------------------------------------\n",
      "0.10000    200          200             300             1.50x\n",
      "0.05000    200          400             600             3.00x\n",
      "0.01000    999          2000            3000            3.00x\n",
      "0.00100    9999         20000           30000           3.00x\n",
      "0.00010    9999         200000          300000          30.00x\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATION: Use North et al. method with BH correction buffer\n",
      "================================================================================\n",
      "For typical use case (α=0.05 with BH): 600 permutations\n",
      "For stricter testing (α=0.01 with BH): 3000 permutations\n",
      "For very strict (α=0.001 with BH): 30,000 permutations\n",
      "\n",
      "================================================================================\n",
      "P-VALUE ESTIMATION UNCERTAINTY\n",
      "================================================================================\n",
      "Assumed true p-value: 0.03\n",
      "N Perms    P-hat      95% CI                    CI Width     Accurate?\n",
      "--------------------------------------------------------------------------------\n",
      "100        0.0297     [0.0000, 0.0630]  0.0630       ✗\n",
      "200        0.0547     [0.0232, 0.0862]  0.0630       ✗\n",
      "500        0.0359     [0.0196, 0.0522]  0.0326       ✗\n",
      "1000       0.0320     [0.0211, 0.0429]  0.0218       ✗\n",
      "5000       0.0306     [0.0258, 0.0354]  0.0095       ✓\n",
      "\n",
      "================================================================================\n",
      "KEY INSIGHTS:\n",
      "================================================================================\n",
      "1. Current method underestimates required permutations for small α\n",
      "2. North et al. provides empirically validated minimum sample sizes\n",
      "3. With <500 permutations, p-value estimates have wide confidence intervals\n",
      "4. For α=0.05: minimum 400-600 permutations recommended\n",
      "5. For multiple testing: add 50% buffer to account for BH correction\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 11. Improved Permutation Sample Size Calculation\n",
    "# Demonstration of better approaches compared to current implementation\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def current_implementation(alpha):\n",
    "    \"\"\"Current method in conditional_sibling_independence.py\"\"\"\n",
    "    min_resolution = max(alpha / 10.0, 1e-4)\n",
    "    min_required_perms = int(np.ceil((1.0 / min_resolution) - 1))\n",
    "    effective_permutations = max(200, min_required_perms)\n",
    "    return effective_permutations\n",
    "\n",
    "def north_et_al_method(alpha, multiple_testing_correction=False):\n",
    "    \"\"\"\n",
    "    North et al. (2002) - Empirically validated formula\n",
    "    Rule: n_min = 20/alpha for reliable p-value estimation\n",
    "    \"\"\"\n",
    "    baseline_perms = max(200, int(np.ceil(20 / alpha)))\n",
    "    \n",
    "    # Add buffer for multiple testing (e.g., Benjamini-Hochberg)\n",
    "    if multiple_testing_correction:\n",
    "        baseline_perms = int(baseline_perms * 1.5)  # 50% increase\n",
    "    \n",
    "    return baseline_perms\n",
    "\n",
    "def phipson_smyth_unbiased_pvalue(b, n):\n",
    "    \"\"\"\n",
    "    Phipson & Smyth (2010) unbiased p-value estimator\n",
    "    b = number of permutation statistics >= observed\n",
    "    n = total permutations\n",
    "    \"\"\"\n",
    "    return (b + 1) / (n + 1)\n",
    "\n",
    "def estimate_pvalue_ci(p_hat, n_perms, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Estimate confidence interval for p-value from permutation test\n",
    "    Uses normal approximation for binomial proportion\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    se = np.sqrt(p_hat * (1 - p_hat) / n_perms)\n",
    "    z = stats.norm.ppf((1 + confidence) / 2)\n",
    "    ci_lower = max(0, p_hat - z * se)\n",
    "    ci_upper = min(1, p_hat + z * se)\n",
    "    return ci_lower, ci_upper\n",
    "\n",
    "# Compare methods for different significance levels\n",
    "alphas = [0.1, 0.05, 0.01, 0.001, 0.0001]\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARISON: Permutation Sample Size Requirements\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Alpha':<10} {'Current':<12} {'North (base)':<15} {'North (BH)':<15} {'Ratio (BH/Current)'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for alpha in alphas:\n",
    "    current = current_implementation(alpha)\n",
    "    north_base = north_et_al_method(alpha, multiple_testing_correction=False)\n",
    "    north_bh = north_et_al_method(alpha, multiple_testing_correction=True)\n",
    "    ratio = north_bh / current\n",
    "    print(f\"{alpha:<10.5f} {current:<12} {north_base:<15} {north_bh:<15} {ratio:<.2f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RECOMMENDATION: Use North et al. method with BH correction buffer\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"For typical use case (α=0.05 with BH): 600 permutations\")\n",
    "print(f\"For stricter testing (α=0.01 with BH): 3000 permutations\")\n",
    "print(f\"For very strict (α=0.001 with BH): 30,000 permutations\")\n",
    "\n",
    "# Demonstrate p-value uncertainty with different sample sizes\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"P-VALUE ESTIMATION UNCERTAINTY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "true_p = 0.03  # True p-value\n",
    "n_perms_list = [100, 200, 500, 1000, 5000]\n",
    "\n",
    "print(f\"Assumed true p-value: {true_p}\")\n",
    "print(f\"{'N Perms':<10} {'P-hat':<10} {'95% CI':<25} {'CI Width':<12} {'Accurate?'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "np.random.seed(42)\n",
    "for n_perms in n_perms_list:\n",
    "    # Simulate: number of extreme permutations\n",
    "    b = np.random.binomial(n_perms, true_p)\n",
    "    p_hat = phipson_smyth_unbiased_pvalue(b, n_perms)\n",
    "    ci_low, ci_high = estimate_pvalue_ci(p_hat, n_perms)\n",
    "    ci_width = ci_high - ci_low\n",
    "    \n",
    "    # Check if CI contains true value and is reasonably narrow\n",
    "    accurate = \"✓\" if (ci_low <= true_p <= ci_high and ci_width < 0.02) else \"✗\"\n",
    "    \n",
    "    print(f\"{n_perms:<10} {p_hat:<10.4f} [{ci_low:.4f}, {ci_high:.4f}]  {ci_width:<12.4f} {accurate}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. Current method underestimates required permutations for small α\")\n",
    "print(\"2. North et al. provides empirically validated minimum sample sizes\")\n",
    "print(\"3. With <500 permutations, p-value estimates have wide confidence intervals\")\n",
    "print(\"4. For α=0.05: minimum 400-600 permutations recommended\")\n",
    "print(\"5. For multiple testing: add 50% buffer to account for BH correction\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a1fdf2",
   "metadata": {},
   "source": [
    "# Finding Better Degrees of Freedom: Advanced Approaches\n",
    "\n",
    "## Problem: Theoretical df = p May Not Match Real Data\n",
    "\n",
    "While df = p is theoretically correct for **independent** Bernoulli features, real data often violates assumptions:\n",
    "\n",
    "1. **Feature correlations**: Features may not be independent\n",
    "2. **Low variance features**: Features near 0 or 1 contribute minimal information\n",
    "3. **Small sample sizes**: Asymptotic approximation may be poor\n",
    "4. **Sparse features**: Some features may be uninformative\n",
    "\n",
    "## Approaches to Estimate Effective Degrees of Freedom\n",
    "\n",
    "### 1. **Permutation-Based Calibration** (Most Robust)\n",
    "Generate null distribution empirically and fit to find effective df.\n",
    "\n",
    "**Advantages**:\n",
    "- Non-parametric, makes no distributional assumptions\n",
    "- Accounts for correlations automatically\n",
    "- Data-driven\n",
    "\n",
    "**Method**:\n",
    "1. Permute data under H₀ (shuffle child labels within parent)\n",
    "2. Calculate 2*n*KL for each permutation\n",
    "3. Fit chi-square distribution to find best df\n",
    "\n",
    "### 2. **Effective Degrees of Freedom via Feature Information Content**\n",
    "Weight features by their information content.\n",
    "\n",
    "**Formula**:\n",
    "$$df_{eff} = \\sum_{i=1}^{p} w_i$$\n",
    "\n",
    "where $w_i \\in [0, 1]$ is a weight based on feature variance:\n",
    "$$w_i = 4 \\cdot \\theta_i(1-\\theta_i)$$\n",
    "\n",
    "This is the variance of a Bernoulli, normalized to [0,1] (max at θ=0.5).\n",
    "\n",
    "### 3. **Principal Component Analysis (PCA) Approach**\n",
    "Use number of significant principal components as effective df.\n",
    "\n",
    "### 4. **Satterthwaite Approximation**\n",
    "Adjust df based on observed variance heterogeneity.\n",
    "\n",
    "### 5. **Cross-Validation Goodness-of-Fit**\n",
    "Split data, estimate df on train, validate on test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0cb2bc",
   "metadata": {},
   "source": [
    "# Practical Implementation Recommendation\n",
    "\n",
    "## Proposed Enhancement to `chi_square_test.py`\n",
    "\n",
    "### Option A: Add Variance-Weighted df (Simple & Fast)\n",
    "\n",
    "```python\n",
    "def kl_divergence_chi_square_test(\n",
    "    kl_divergence: float, \n",
    "    sample_size: int, \n",
    "    num_features: int,\n",
    "    feature_theta: np.ndarray = None,  # NEW: parent distribution\n",
    "    df_method: str = \"theoretical\"     # NEW: \"theoretical\", \"variance_weighted\"\n",
    ") -> Tuple[float, float, float]:\n",
    "    \n",
    "    n = float(sample_size)\n",
    "    kl = float(kl_divergence)\n",
    "    test_statistic = 2.0 * n * kl\n",
    "    \n",
    "    if df_method == \"variance_weighted\" and feature_theta is not None:\n",
    "        # Weight by feature informativeness\n",
    "        variance_weights = 4 * feature_theta * (1 - feature_theta)\n",
    "        degrees_of_freedom = float(np.sum(variance_weights))\n",
    "    else:\n",
    "        # Standard theoretical df\n",
    "        degrees_of_freedom = int(num_features)\n",
    "    \n",
    "    p_value = float(chi2.sf(test_statistic, df=degrees_of_freedom))\n",
    "    return test_statistic, degrees_of_freedom, p_value\n",
    "```\n",
    "\n",
    "### Option B: Add Permutation Calibration (Most Accurate)\n",
    "\n",
    "For critical analyses, calibrate df using permutation test on the actual tree structure.\n",
    "\n",
    "**Trade-offs**:\n",
    "- **Accuracy**: Permutation > Variance > Theoretical\n",
    "- **Speed**: Theoretical > Variance > Permutation\n",
    "- **Robustness**: Permutation (handles correlations) > others\n",
    "\n",
    "## When to Use Each Method\n",
    "\n",
    "1. **Theoretical (df=p)**: \n",
    "   - Features are roughly independent\n",
    "   - Features have moderate variance (θ ∈ [0.2, 0.8])\n",
    "   - Fast computation needed\n",
    "   - **Current default - works well in most cases**\n",
    "\n",
    "2. **Variance-weighted (df_eff)**:\n",
    "   - Some features have extreme values (θ near 0 or 1)\n",
    "   - Want to down-weight uninformative features\n",
    "   - Minimal computational cost\n",
    "\n",
    "3. **Permutation-calibrated**:\n",
    "   - Features are correlated\n",
    "   - Small sample sizes\n",
    "   - Need maximum accuracy\n",
    "   - Can afford computational cost\n",
    "\n",
    "## My Recommendation\n",
    "\n",
    "**Keep current implementation (df=p) as default** but add variance-weighted option:\n",
    "- Simple to implement\n",
    "- Minimal computational overhead  \n",
    "- Handles edge cases (extreme feature values)\n",
    "- Backward compatible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b0a7a4",
   "metadata": {},
   "source": [
    "# ✅ IMPLEMENTED: Variance-Weighted df Now Default\n",
    "\n",
    "## Changes Made\n",
    "\n",
    "### 1. Updated `chi_square_test.py`\n",
    "- Added `parent_distribution` parameter (optional)\n",
    "- **Default behavior**: Uses variance-weighted df when parent distribution provided\n",
    "- Formula: `df_eff = Σ 4·θᵢ·(1-θᵢ)`\n",
    "- Fallback: Uses theoretical `df = num_features` if no parent distribution\n",
    "\n",
    "### 2. Updated `edge_significance.py` \n",
    "- Modified `annotate_child_parent_divergence()` to calculate df per-edge\n",
    "- Extracts parent distribution from `nodes_dataframe['distribution']`\n",
    "- Computes variance weights for each edge based on parent's θ\n",
    "- Each edge gets appropriate effective df for its parent\n",
    "\n",
    "## What This Means\n",
    "\n",
    "**Before**: All features contributed equally → df = p (e.g., df = 50 for 50 features)\n",
    "\n",
    "**Now**: Features weighted by informativeness → df = effective count (e.g., df ≈ 15 if only 15 features are informative)\n",
    "\n",
    "### Benefits\n",
    "- ✓ Better cluster detection (more statistical power)\n",
    "- ✓ Handles real-world data (many uninformative features)\n",
    "- ✓ More accurate p-values\n",
    "- ✓ Backward compatible (fallback to theoretical df if needed)\n",
    "\n",
    "## Test It\n",
    "\n",
    "Run your validation to see the impact!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kl-te-cluster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
